# Intermediate Machine Learning
This course is estimated to last 4 hours, builds on Python, and prepares for the following courses:
- [Feature Engineering](https://www.kaggle.com/learn/feature-engineering)
- [Time Series](https://www.kaggle.com/learn/time-series)

A `requirements.txt` file is included. It lists all the required dependencies to run the coding exercises present in this course. To install it, please execute: `pip install -r requirements.txt`

## [Lesson 1: Introduction](https://www.kaggle.com/code/alexisbcook/introduction)
This course will go trough more realistic usecases than the [Introduction to Machine Learning](https://github.com/jmtc7/kaggle-courses/tree/main/00_intro_to_machine_learning) one, with topics including:
- Handling of real world data, with missing values and categorical variables
- Designing pipelines to improve ML models
- Advanced validation methods (cross-validation)
- Build and use state-of-the-art ML models (XGBoost)
- Avoiding common ML issues (leakage)

There will be some coding exercises, which will use the data from [Kaggle's Housing Prices Competition](https://www.kaggle.com/c/home-data-for-ml-course). The predictions will be submitted to see how we rise in the leaderboard as we improve our model.


## [Lesson 2: Missing Values](https://www.kaggle.com/code/alexisbcook/missing-values)
TODO


## [Lesson 3: Categorical Variables](https://www.kaggle.com/code/alexisbcook/categorical-variables)
TODO


## [Lesson 4: Pipelines](https://www.kaggle.com/code/alexisbcook/pipelines)
TODO


## [Lesson 5: Cross Validation](https://www.kaggle.com/code/alexisbcook/cross-validation)
TODO


## [Lesson 6: XGBoost](https://www.kaggle.com/code/alexisbcook/xgboost)
TODO


## [Lesson 7: Data Leakage](https://www.kaggle.com/code/alexisbcook/data-leakage)
TODO
